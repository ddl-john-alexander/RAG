{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8364202-a753-4e7b-9a10-f25380c86527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI, Cohere\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "#\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time,sleep\n",
    "import openai\n",
    "import tiktoken\n",
    "#\n",
    "import os\n",
    "import json\n",
    "#\n",
    "import io\n",
    "#\n",
    "import mlflow\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20365fa0-f4fb-45b0-a043-f438f8f6f5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Openai key: ········\n",
      "Enter Cohere key: ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "# get a token: https://platform.openai.com/account/api-keys\n",
    "os.environ['OPENAI_API_KEY'] = getpass(\"Enter Openai key:\")\n",
    "# get a new token: https://dashboard.cohere.ai/\n",
    "os.environ['COHERE_API_KEY'] = getpass(\"Enter Cohere key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45145ca5-a76c-4f27-840c-74a2ed52c667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='/mnt/code/data/disease_components.csv',source_column=\"link\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6212dc5-6711-4f48-b2ff-1cc5f59b7c63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1183 1183\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to hold embeddings with model names as keys\n",
    "embeddings_dict = {}\n",
    "\n",
    "metadatas = []\n",
    "texts = []\n",
    "for row in data:\n",
    "  metadatas.append(row.metadata)\n",
    "  texts.append(row.page_content)\n",
    "print(len(metadatas),len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0db99fb-22c0-46a4-9f5f-cc165044ac21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question enclosed within  3 backticks at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Please provide an answer which is factually correct and based on the information retrieved from the vector store.\n",
    "Please also mention any quotes supporting the answer if any present in the context supplied within two double quotes \"\" .\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n",
    "#\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ff7022b-abd2-4fc9-9e20-484ead4582fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to load model based on name\n",
    "def get_qa_chain(llm_name='OpenAI', embedding_model_name='OpenAI'):\n",
    "    \n",
    "    qa = None\n",
    "    doc_store = embeddings_dict.get(embedding_model_name)\n",
    "    \n",
    "    if llm_name == 'Cohere':\n",
    "        rag_llm = Cohere(model=\"command\",\n",
    "                         temperature=0,\n",
    "                         cohere_api_key=os.environ[\"COHERE_API_KEY\"])\n",
    "    else:\n",
    "        rag_llm = ChatOpenAI(model_name='gpt-3.5-turbo-16k',\n",
    "                             openai_api_key=os.environ[\"COHERE_API_KEY\"],\n",
    "                             temperature=0)\n",
    "            \n",
    "        return RetrievalQA.from_chain_type(llm=rag_llm,\n",
    "                                  chain_type=\"stuff\",\n",
    "                                  chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                  retriever=doc_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                  return_source_documents=True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88bfa402-8090-4967-b17f-6c0cd07157ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_embedding(embedding_model_name='HuggingFace'):\n",
    "    \n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    \n",
    "    if embedding_model_name == 'OpenAI':\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "           \n",
    "    elif embedding_model_name =='BGE':\n",
    "        embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en\",\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )\n",
    "    else:\n",
    "         embeddings = HuggingFaceEmbeddings(model_kwargs = model_kwargs,\n",
    "                                            encode_kwargs = encode_kwargs,\n",
    "                                           )\n",
    "        \n",
    "    store = Qdrant.from_texts(texts,\n",
    "                              metadatas=metadatas,\n",
    "                              embedding=embeddings,\n",
    "                              path=\"/mnt/data/RAG-mktg/\",\n",
    "                              prefer_grpc=True,\n",
    "                              collection=f\"{embedding_model_name}_medical_qa_search\")\n",
    "    \n",
    "    embeddings_dict[embedding_model_name] = store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec9cd44-61ad-478d-86d9-6a7c7b6bb109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your search params\n",
    "llms = ('OpenAI', 'Cohere')  # Model names\n",
    "# embedding_models = ('HuggingFace', 'OpenAI', 'BGE')  # Embedding names\n",
    "# embedding_models = ('HuggingFace', 'BGE')  # Embedding names\n",
    "embedding_models = ('BGE')  # Embedding names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58369f7e-8a78-4300-bbf1-ffdad457d4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_dict = {}  # Dictionary to hold the Qdrant stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ebaa89-fb97-4010-8fc0-0833b6df1953",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for HuggingFace\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2dd44d18504e9289061f836167d26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)99753/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6524812077a84018ba25006f35d33b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a606644ff5d4d15b6915d5234a5ba7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0cdb299753/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83f0f7cab344a3ca1ee1e4396cf3250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)db299753/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a1aa8c768f4dbf8c83c83b0960d386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a765ee5649346e59778e1025cf34312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)753/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd1dc97a96049099ccaa7df7ae7345e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e1463b46a24235b04a2822f8114f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba49ff7113848f183ca89684b6ceb66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23d2fbf5906451dba0bef9ec47cddb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)99753/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f6b3ea0b80440fbe50725180ff5585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041ca9d2e672427ebf6e2ecf7d0f6b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9753/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e905dd754324436bf88672223924950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)0cdb299753/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d099460fa27646b2808e16a437bc9bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)b299753/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for BGE\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HuggingFaceBgeEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m embedding_models:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating embeddings for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mcompute_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mcompute_embedding\u001b[0;34m(embedding_model_name)\u001b[0m\n\u001b[1;32m      7\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m embedding_model_name \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBGE\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceBgeEmbeddings\u001b[49m(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBAAI/bge-small-en\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m                                   model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[1;32m     12\u001b[0m                                   encode_kwargs\u001b[38;5;241m=\u001b[39mencode_kwargs\n\u001b[1;32m     13\u001b[0m                                  )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m      embeddings \u001b[38;5;241m=\u001b[39m HuggingFaceEmbeddings(model_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs,\n\u001b[1;32m     16\u001b[0m                                         encode_kwargs \u001b[38;5;241m=\u001b[39m encode_kwargs,\n\u001b[1;32m     17\u001b[0m                                        )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HuggingFaceBgeEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute embeddings\n",
    "for model_name in embedding_models:\n",
    "    print(f\"Generating embeddings for {model_name}\")\n",
    "    compute_embedding(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403ee56-1330-4328-b8c5-4f9d680c375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to evaluate the qa system we generated a few relevant questions and answers\n",
    "eval_questions = [\n",
    "    \"I have persistent back pain since 4 weeks,I workout but havent had any sports injury.What might be the cause of the back pain?\",\n",
    "    \"I have shortness of breath and frequently feel nauseated and tired.What can be the possible cause?\",\n",
    "    \"My 12 year old son has Poor coordination Unsteady walk and a tendency to stumble while walking and poor coordination between two hands.What might be the possible cuase?\",\n",
    "    \"What is Baby acne ?\",\n",
    "    \"What is Botulism ?\",\n",
    "]\n",
    "\n",
    "eval_answers = [\n",
    "    \"From the symptoms mentioned you might have a disloacted disk\",  # incorrect answer\n",
    "    \"You might have asthama.\",  # incorrect answer\n",
    "    \" Movement and coordination problems associated with cerebral palsy.Please consult a doctor for better diagnosis.\",\n",
    "    \"Baby acne is small, inflamed bumps on a baby's face, neck, back or chest.\",\n",
    "    \"Botulism is a rare and potentially fatal illness caused by a toxin produced by the bacterium Clostridium botulinum.\",\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    {\"query\": q, \"ground_truths\": [eval_answers[i]]}\n",
    "    for i, q in enumerate(eval_questions)\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b74ed-6ab8-4c08-8f2b-89f1ec13ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all the evaluation chains\n",
    "from ragas.langchain.evalchain import RagasEvaluatorChain\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "# create evaluation chains\n",
    "faithfulness_chain = RagasEvaluatorChain(metric=faithfulness)\n",
    "answer_rel_chain = RagasEvaluatorChain(metric=answer_relevancy)\n",
    "context_rel_chain = RagasEvaluatorChain(metric=context_relevancy)\n",
    "context_recall_chain = RagasEvaluatorChain(metric=context_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca1b6de-581d-4f42-887b-0886cbf60657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all combinations of these parameters\n",
    "search_space = list(itertools.product(llms, embedding_models))\n",
    "search_spacech_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015a710-0cbb-4b7a-95ac-e46fec4054b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model_name, embedding_name):\n",
    "    \n",
    "    qa_chain = get_qa_chain(model_name, embedding_name)\n",
    "    \n",
    "    predictions = qa.batch(examples)\n",
    "    faithfulness_scores = faithfulness_chain.evaluate(examples, predictions)\n",
    "    context_recall_scores = context_recall_chain.evaluate(examples, predictions)\n",
    "    answer_rel_scores = answer_rel_chain.evaluate(examples, predictions)\n",
    "    context_rel_scores = context_rel_chain.evaluate(examples, predictions)\n",
    "    \n",
    "    # Combine scores into a dataframe\n",
    "    df_scores = pd.DataFrame({\n",
    "        'faithfulness': faithfulness_scores,\n",
    "        'context_recall': context_recall_scores,\n",
    "        'answer_relevance': answer_rel_scores,\n",
    "        'context_relevance': context_rel_scores\n",
    "    })\n",
    "    \n",
    "    # Calculate the median of each column\n",
    "    median_scores = df_scores.median().to_dict()\n",
    "    \n",
    "    # Return the results of the experiment \n",
    "    return median_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2c9fcf-72fe-48f5-bfd1-fa0ab46b14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and set the experiment name\n",
    "experiment_name = \"RAG_Parameter_search\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Iterate through each combination and execute the MLflow runs\n",
    "for llm_name, embedding_model_name in search:\n",
    "    run_name = f\"{llm_name}_{embedding_model_name}_run\"\n",
    "    print(f'run_name={run_name}')\n",
    "    # Log parameters\n",
    "    print(f\"model : {llm_name}\")\n",
    "    print(f\"embedding : {embedding_model_name}\")\n",
    "   # Run the experiment\n",
    "    results = run_experiment(llm_name, embedding_model_name)\n",
    "    # Log results\n",
    "    for key, value in results.items():\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfab4561-efc0-4fb1-8d18-9dc95fe140d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create and set the experiment name\n",
    "# experiment_name = \"RAG_Parameter_search\"\n",
    "# mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# # Iterate through each combination and execute the MLflow runs\n",
    "# for llm_name, embedding_model_name in search:\n",
    "#     run_name = f\"{llm_name}_{embedding_model_name}_run\"\n",
    "#     with mlflow.start_run(run_name=run_name):\n",
    "#         # Log parameters\n",
    "#         mlflow.log_param(\"model\", llm_name)\n",
    "#         mlflow.log_param(\"embedding\", embedding_model_name)\n",
    "\n",
    "#         # Run the experiment\n",
    "#         results = run_experiment(llm_name, embedding_model_name)\n",
    "\n",
    "#         # Log results\n",
    "#         for key, value in results.items():\n",
    "#             mlflow.log_metric(key, value)\n",
    "\n",
    "#         # End the run\n",
    "#         mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
