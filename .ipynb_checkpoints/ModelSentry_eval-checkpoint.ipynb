{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2d1ea-d50e-4930-8065-ddf5acef315d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, OpenAIEmbeddings, HuggingFaceBgeEmbeddings\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores.qdrant import Qdrant\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain import PromptTemplate\n",
    "#\n",
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "from transformers import pipeline\n",
    "from getpass import getpass\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import openai\n",
    "import tiktoken\n",
    "#\n",
    "import os\n",
    "import json\n",
    "#\n",
    "import io\n",
    "#\n",
    "import mlflow\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd66ad-7ec3-4d89-8991-513029fd226d",
   "metadata": {},
   "source": [
    "**First lets start mlflow. Launch a terminal and run mlflow server --host 127.0.0.1 --port 8080 before running the cells below. Change the IP address and port if you need to**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c3f73a99-0300-4007-8b40-35a3eabea81f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass(\"Enter Openai key:\")\n",
    "os.environ['ANTHROPIC_API_KEY'] = getpass(\"Enter Anthropic key:\")\n",
    "os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/mnt/data/RAG-mktg/model_cache/'\n",
    "# Change the IP address and port if you are using a different one\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "557fde03-e6d0-4f23-b280-85e3e56f3dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "loader = CSVLoader(file_path='/mnt/code/data/disease_components.csv',source_column=\"link\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78aa47c-d20f-4023-a690-7d990f9e9409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dictionary to hold embeddings with model names as keys\n",
    "embeddings_dict = {}\n",
    "\n",
    "metadatas = []\n",
    "texts = []\n",
    "for row in data:\n",
    "  metadatas.append(row.metadata)\n",
    "  texts.append(row.page_content)\n",
    "print(len(metadatas),len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ca295-85e2-4ab2-8d5f-d8fd30ea8513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Use the following pieces of context to answer the question enclosed within  3 backticks at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Please provide an answer which is factually correct and based on the information retrieved from the vector store.\n",
    "Please also mention any quotes supporting the answer if any present in the context supplied within two double quotes \"\" .\n",
    "\n",
    "{context}\n",
    "\n",
    "QUESTION:```{question}```\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"question\"])\n",
    "#\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644ba00c-bc52-48a5-bd01-1c1f88b2a213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get a qa chain based on the llm and embedding model inputs\n",
    "def get_qa_chain(llm_name, embedding_model_name='OpenAI'):\n",
    "    \n",
    "    qa = None\n",
    "    doc_store = embeddings_dict.get(embedding_model_name)\n",
    "    \n",
    "    if llm_name == 'Anthropic':\n",
    "        rag_llm = ChatAnthropic(temperature=0,\n",
    "                         anthropic_api_key=os.environ[\"ANTHROPIC_API_KEY\"])\n",
    "        \n",
    "    elif llm_name =='gpt-4':\n",
    "        rag_llm = ChatOpenAI(model_name='gpt-4',\n",
    "                             openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                             temperature=0)\n",
    "        \n",
    "    else:\n",
    "        rag_llm = ChatOpenAI(model_name='gpt-3.5-turbo-16k',\n",
    "                             openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "                             temperature=0)\n",
    "            \n",
    "    return RetrievalQA.from_chain_type(llm=rag_llm,\n",
    "                                       chain_type=\"stuff\",\n",
    "                                       chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                       retriever=doc_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "                                       return_source_documents=True\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3906a5f-f779-439a-8799-94c144d78374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute embeddings\n",
    "def compute_embedding(embedding_model_name='HuggingFace'):\n",
    "    \n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    \n",
    "    if embedding_model_name == 'OpenAI':\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "           \n",
    "    elif embedding_model_name =='BGE':\n",
    "        embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-small-en\",\n",
    "                                      model_kwargs=model_kwargs,\n",
    "                                      encode_kwargs=encode_kwargs\n",
    "                                     )\n",
    "    else:\n",
    "         embeddings = HuggingFaceEmbeddings(model_kwargs = model_kwargs,\n",
    "                                            encode_kwargs = encode_kwargs,\n",
    "                                           )\n",
    "        \n",
    "    store = Qdrant.from_texts(texts,\n",
    "                              metadatas=metadatas,\n",
    "                              embedding=embeddings,\n",
    "                              location=\":memory:\",\n",
    "                              prefer_grpc=True,\n",
    "                              collection=f\"{embedding_model_name}_medical_qa_search\")\n",
    "    \n",
    "    embeddings_dict[embedding_model_name] = store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b88cea-1a86-4606-b993-831d34dcb1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your search params\n",
    "llms = ('OpenAI', 'Anthropic')  # Model names\n",
    "embedding_models = ('BGE',)  # Embedding names , lets just evaluate the models for now\n",
    "# embedding_models = ('HuggingFace', 'OpenAI', 'BGE')  # Embedding model options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92332e40-a198-4ffc-a095-8c976084fb83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_dict = {}  # Dictionary to hold the Qdrant stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5301ad75-ed48-43b1-b207-1203a6c22d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets compute the embeddings for the embedding models specified above\n",
    "for model_name in embedding_models:\n",
    "    print(f\"Generating embeddings for {model_name}\")\n",
    "    compute_embedding(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70552f40-ee59-4be9-a452-e98a8861e33d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data frame of questions that we will use to evaluate the RAG pipeline\n",
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"questions\": [\n",
    "            \"I have persistent back pain since 4 weeks,I workout but havent had any sports injury.What might be the cause of the back pain?\",\n",
    "            \"I have shortness of breath and frequently feel nauseated and tired.What can be the possible cause?\",\n",
    "            \"My 12 year old son has Poor coordination Unsteady walk and a tendency to stumble while walking and poor coordination between two hands.What might be the possible cuase?\",\n",
    "            \"What is Baby acne ?\",\n",
    "            \"What is Botulism ?\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bc3ff-cae6-45a2-aad2-396e21bbc46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets setip a faithfulness metric\n",
    "from mlflow.metrics.genai import faithfulness, EvaluationExample\n",
    "\n",
    "# Create a good and bad example for faithfulness in the context of this problem\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"What is the cause of Achalasia?\",\n",
    "        output=\"We don't know the exact cause of achalasia and there are many theories about its cause \",\n",
    "        score=2,\n",
    "        justification=\"The output provides an answer that does not use all the information that was present in the context and therefore does not convey a comprehensive answer.\",\n",
    "        grading_context={\n",
    "            \"context\": \"The exact cause of achalasia is poorly understood. Researchers suspect it may be caused by a loss of nerve cells in the esophagus. There are theories about what causes this, but viral infection or autoimmune responses have been suspected. Very rarely, achalasia may be caused by an inherited genetic disorder or infection\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"What is the cause of Achalasia?\",\n",
    "        output=\"The exact cause is not well understood and there are many theories about its cause. Researchers suspect it may be caused by nerve cell loss in the esophagus but is rarely caused by an inherited genetic disorder\",\n",
    "        score=5,\n",
    "        justification=\"The output provides an answer using most of the information provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"The exact cause of achalasia is poorly understood. Researchers suspect it may be caused by a loss of nerve cells in the esophagus. There are theories about what causes this, but viral infection or autoimmune responses have been suspected. Very rarely, achalasia may be caused by an inherited genetic disorder or infection\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(model=\"openai:/gpt-4\", examples=faithfulness_examples)\n",
    "print(faithfulness_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0fad0-6cf3-4046-8fc5-7dc1891689ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets setup a relevance metric\n",
    "from mlflow.metrics.genai import relevance, EvaluationExample\n",
    "\n",
    "# Create a good and bad example for relevance in the context of this problem\n",
    "relevance_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"At what age is it most common to rupture an Achilles tendon?\",\n",
    "        output=\"Your Achilles tendon helps you point your foot downward, rise on your toes and push off your foot as you walk. You rely on it virtually every time you walk and move your foot.The peak age for Achilles tendon rupture is 30 to 40 and more likely to occur in men than women\",\n",
    "        score=2,\n",
    "        justification=\"The output provides an answer that has the information but also some irrelevant information about what the Achilles tendon does\",\n",
    "        grading_context={\n",
    "            \"context\": \"Factors that may increase your risk of Achilles tendon rupture include:Age. The peak age for Achilles tendon rupture is 30 to 40.\\', \\'Sex. Achilles tendon rupture is up to five times more likely to occur in men than in women.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"At what age is it most common to rupture an Achilles tendon?\",\n",
    "        output=\"The peak age for Achilles tendon rupture is 30 to 40 with men being five times more likely to rupture an Achilles tendon than women\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a relevant answer to the question being asked by extracting the relevant information from the context\",\n",
    "        grading_context={\n",
    "            \"context\": \"Factors that may increase your risk of Achilles tendon rupture include:Age. The peak age for Achilles tendon rupture is 30 to 40.\\', \\'Sex. Achilles tendon rupture is up to five times more likely to occur in men than in women.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "relevance_metric = relevance(model=\"openai:/gpt-4\", examples=relevance_examples)\n",
    "print(relevance_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1bece135-a8f2-4233-8109-0c87152eb019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function that returns the response from the RAG for the evaluation dataset\n",
    "def model(input_df):\n",
    "    answer = []\n",
    "    for index, row in input_df.iterrows():\n",
    "        answer.append(qa(row[\"questions\"]))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eab8a2-67f3-4551-9a30-5b406d3ef358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets run the evaluation for the llm-embedding model combinations\n",
    "\n",
    "qa = None\n",
    "df_metrics = pd.DataFrame()\n",
    "\n",
    "# Iterate through each combination and execute the evaluations\n",
    "for llm_name, embedding_model_name in search_space:\n",
    "    run_name = f\"{llm_name}_{embedding_model_name}_run\"\n",
    "    print(f'run_name={run_name}')\n",
    "    # Log parameters\n",
    "    print(f\"model : {llm_name}\")\n",
    "    print(f\"embedding : {embedding_model_name}\")\n",
    "    qa = get_qa_chain(llm_name, embedding_model_name)\n",
    "    # Run the evaluation\n",
    "    results = mlflow.evaluate(\n",
    "    model,\n",
    "    eval_df,\n",
    "    model_type=\"question-answering\",\n",
    "    evaluators=\"default\",\n",
    "    predictions=\"result\",\n",
    "    extra_metrics=[faithfulness_metric, relevance_metric, mlflow.metrics.latency()],\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"inputs\": \"questions\",\n",
    "            \"context\": \"source_documents\",\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    metrics_series = pd.Series(results.metrics, name=f'{llm_name}_{embedding_model_name}')\n",
    "    df_metrics = df_metrics.append(metrics_series)\n",
    "    \n",
    "df_metrics = df_metrics.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "763e54f8-b99f-465a-b8b8-f64e286df860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt-3.5_BGE</th>\n",
       "      <th>Anthropic_BGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>faithfulness/v1/mean</th>\n",
       "      <td>2.60</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness/v1/variance</th>\n",
       "      <td>3.84</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness/v1/p90</th>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevance/v1/mean</th>\n",
       "      <td>2.20</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevance/v1/variance</th>\n",
       "      <td>2.16</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevance/v1/p90</th>\n",
       "      <td>4.00</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          gpt-3.5_BGE  Anthropic_BGE\n",
       "faithfulness/v1/mean             2.60           4.60\n",
       "faithfulness/v1/variance         3.84           0.24\n",
       "faithfulness/v1/p90              5.00           5.00\n",
       "relevance/v1/mean                2.20           3.60\n",
       "relevance/v1/variance            2.16           1.84\n",
       "relevance/v1/p90                 4.00           4.60"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics # The Anthropic model does better than the gpt-3.5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00409f42-dee7-4c1b-bb66-3742b7539ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets now log these metrics in Domino\n",
    "mlflow.set_tracking_uri(os.environ['MLFLOW_TRACKING_URI'])\n",
    "# Define the experiment name\n",
    "experiment_name = 'RAG eval'\n",
    "mlflow.set_experiment(experiment_name)\n",
    "for column in df_metrics:\n",
    "    with mlflow.start_run(run_name=column):\n",
    "        for metric_name, metric_value in df_metrics[column].items():\n",
    "            # Log the metric\n",
    "            mlflow.log_metric(metric_name, metric_value)"
   ]
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
